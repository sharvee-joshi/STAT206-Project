# Introduction

Air pollution is not only an environmental concern, but also a public health one. There is substantial evidence that indicates a link in adverse health outcomes, including an increase in mortality rates. Using quantitative analysis, statisticians in the past have been able to determine the exact link as well as use a multitude of methods to study these results.

One example is seen in the article titled: *"Instabilities of Regression Estimates Relating Air Pollution to Mortality"*. The article aimed to explore the relationship between air pollution and mortality rates with the use of regression-based estimates. The authors use a various number of factors, such as rain, temperature, different levels of toxic chemicals, to determine a linear model between air pollution and mortality rates. They used ridge regression to create better coefficient estimates due to the underlying issues the data faced.

## Objective

The purpose of this report is to replicate the analyses that were conducted in the original study, using techniques that we have discussed in this class, as well as the use of modern technology. An important note is that the article was written in the 1970s, and the data and statistical methods were most likely conducted during that time. We are aware of the advanced technology we have now, and we hope to compare our output versus the original output, and perhaps observe better results. We also hope to learn more about statistical modeling as well as contribute to a better understanding of how methodologies change over time. Finally, we hope to truly understand and do our best to interpret the relationship between air pollution and mortality rates.

# Literature Review

# Methodology

## Data Description

## Regression Models

## Assumptions

## Reproduction Plan

# Implementation

## Exploratory Data Analysis

```{r}

# pulling the data
library("bestglm")
data <- mcdonald

head(data)
```

## Multiple Linear Regression

## Ridge Regression


```{r}
library(glmnet)
library(MASS)

fit.ols <- lm(MORT ~ ., data = mcdonald)
summary(fit.ols)

```

```{r}
plot(fit.ols)
plot(fit.ridge)
```


Maybe consider this instead

```{r}
#Standardize the predictors
predictors <- scale(mcdonald[, -which(names(mcdonald) == "MORT")])
response <- mcdonald$MORT

#OLS Regression
ols_model <- lm(MORT ~ ., data = mcdonald)
summary(ols_model)

#ridge regression
lambda_seq <- 10^seq(10, -2, length = 100)
ridge_cv <- cv.glmnet(predictors, response, alpha = 0, lambda = lambda_seq)
opt_lambda <- ridge_cv$lambda.min
cat("Optimal Lambda from Ridge:", opt_lambda, "\n")

# Refit ridge with the optimal lambda
ridge_model <- glmnet(predictors, response, alpha = 0, lambda = opt_lambda)
ridge_coefs <- as.data.frame(as.matrix(coef(ridge_model)))
print(ridge_coefs)

#PCR
pcr_model <- pcr(MORT ~ ., data = data, scale = TRUE, validation = "CV")
summary(pcr_model)

#Compare models and predictions
ridge_preds <- predict(ridge_model, newx = predictors)
ols_preds <- predict(ols_model, newdata = data)

# Residual Analysis
par(mfrow = c(1, 2))
plot(response, ridge_preds, main = "Ridge: Observed vs Predicted", xlab = "Observed", ylab = "Predicted")
abline(0, 1)
plot(response, ols_preds, main = "OLS: Observed vs Predicted", xlab = "Observed", ylab = "Predicted")
abline(0, 1)

#address multicollinearity
library(car)
vif_values <- vif(ols_model)
cat("VIF for OLS:\n")
print(vif_values)

#check for improvements
ridge_vif <- 1 / (1 - ridge_cv$cvm[which.min(ridge_cv$cvm)])
cat("Approx. Ridge VIF:\n", ridge_vif)

#Refit Ridge 
#Optionally, choose a reduced set of principal components
pcr_components <- 9 
pcr_preds <- predict(pcr_model, ncomp = pcr_components, newdata = data)
plot(response, pcr_preds, main = "PCR: Observed vs Predicted", xlab = "Observed", ylab = "Predicted")
abline(0, 1)

```

```{r}
#Compute VIF for OLS
library(car)
vif_ols <- vif(ols_model)
cat("VIF for OLS:\n")
print(vif_ols)

#Approximate "Ridge VIF" 
ridge_vif <- 1 / (1 - ridge_cv$cvm[which.min(ridge_cv$cvm)])
cat("Approximate VIF for Ridge:\n", ridge_vif)

#Compare Coefficients
ols_coefs <- coef(ols_model)[-1]  
ridge_coefs_opt <- as.vector(coef(ridge_model))[-1]  #
coef_comparison <- data.frame(
  Variable = names(ols_coefs),
  OLS_Coefficients = ols_coefs,
  Ridge_Coefficients = ridge_coefs_opt
)
print(coef_comparison)
```


```{r}

ols_preds <- predict(ols_model, newdata = data)
ridge_preds <- predict(final_ridge_model, newx = predictors)
ols_mse <- mean((response - ols_preds)^2)
ridge_mse <- mean((response - ridge_preds)^2)
cat("OLS MSE:", ols_mse, "\nRidge MSE:", ridge_mse, "\n")


```

Let't take a look at the multicollinearity

```{r}
cor_matrix <- cor(mcdonald[, -which(names(mcdonald) == "MORT")])
heatmap(cor_matrix, symm = TRUE, main = "Correlation Heatmap")
```

Compile and make it efficient
Step 1
```{r}
#load data
response <- mcdonald$MORT
predictors <- as.matrix(mcdonald[, -which(names(mcdonald) == "MORT")])
```

Step 2: Optimal Lambda
```{r}
# Cross-validation for ridge regression
ridge_cv <- cv.glmnet(predictors, response, alpha = 0)
opt_lambda <- ridge_cv$lambda.min
cat("Optimal Lambda:", opt_lambda, "\n")
```

Step 3: Fit Ridge Model 
```{r}
ridge_model <- glmnet(predictors, response, alpha = 0, lambda = opt_lambda)

# Extract coefficients
ridge_coefs <- as.data.frame(as.matrix(coef(ridge_model)))
ridge_coefs$Variable <- rownames(ridge_coefs)
colnames(ridge_coefs) <- c("Coefficients", "Variable")
ridge_coefs <- ridge_coefs[order(abs(ridge_coefs$Coefficients), decreasing = TRUE), ]  # Optional: Sort by magnitude
print(ridge_coefs)

```

Step 4: Compare OLS and Ridge Coeffiecients

```{r}
ols_model <- lm(MORT ~ ., data = mcdonald)
ols_coefs <- as.data.frame(coef(ols_model))
colnames(ols_coefs) <- "OLS_Coefficients"
ols_coefs$Variable <- rownames(ols_coefs)

# Merge 
coef_comparison <- merge(ols_coefs, ridge_coefs, by = "Variable", all = TRUE)
print(coef_comparison)
```

Step 5: Multicollinearity Analysis

```{r}
library(magrittr)  # Ensure magrittr is loaded

# Compute VIF for OLS
vif_values <- car::vif(ols_model)
vif_df <- data.frame(Variable = names(vif_values), VIF = vif_values)

# Optional: Compute approximate VIF for ridge
ridge_vif <- function(predictors, lambda) {
  diag(solve(t(predictors) %*% predictors + diag(lambda, ncol(predictors)))) %>% sum()
}

ridge_vif_value <- ridge_vif(predictors, opt_lambda)
cat("Approximate Ridge VIF:", ridge_vif_value, "\n")

# Compare VIF values
print(vif_df)

```

Step 6: Correlation Heatmaps

```{r}
# Compute correlation of original predictors (OLS)
ols_cor <- cor(predictors)
heatmap(ols_cor, symm = TRUE, main = "OLS Correlation Heatmap")

# Compute Ridge-transformed predictors
ridge_coefs <- as.matrix(coef(ridge_model, s = opt_lambda))[-1, , drop = FALSE]  # Exclude intercept
ridge_transformed <- scale(predictors) %*% ridge_coefs

# Ensure ridge_transformed is a matrix with multiple columns
if (ncol(ridge_transformed) < 2) {
  stop("Ridge transformation resulted in insufficient variables for correlation matrix.")
}

# Compute correlation of Ridge-transformed predictors
ridge_cor <- cor(ridge_transformed)
heatmap(ridge_cor, symm = TRUE, main = "Ridge Correlation Heatmap")


```

Step 7: Predictive Accuracy 
```{r}
# OLS predictions and RMSE
ols_preds <- predict(ols_model, newdata = as.data.frame(predictors))
ols_rmse <- sqrt(mean((ols_preds - response)^2))

# Ridge predictions and RMSE
ridge_preds <- predict(ridge_model, newx = predictors, s = opt_lambda)
ridge_rmse <- sqrt(mean((ridge_preds - response)^2))

# Display results
rmse_df <- data.frame(Model = c("OLS", "Ridge"), RMSE = c(ols_rmse, ridge_rmse))
print(rmse_df)

```

Step 8: Ridge Trace Plot

```{r}
ridge_model <- glmnet(predictors, response, alpha = 0)

coef_matrix <- as.matrix(coef(ridge_model))
print(dim(coef_matrix))  # Check that you now have multiple columns

matplot(
  log(ridge_model$lambda), t(coef_matrix[-1, ]), 
  type = "l", lty = 1, col = 1:nrow(coef_matrix),
  xlab = "Log(Lambda)", ylab = "Coefficients",
  main = "Ridge Trace Plot"
)
```
```{r}
# Extract coefficients for the optimal lambda
opt_lambda_index <- which.min(abs(ridge_model$lambda - opt_lambda))  # Find closest lambda index
coef_opt_lambda <- coef(ridge_model, s = opt_lambda)                # Extract coefficients

# Convert to a data frame for easier manipulation
coef_df <- as.data.frame(as.matrix(coef_opt_lambda))
coef_df$Variable <- rownames(coef_opt_lambda)
rownames(coef_df) <- NULL  # Clean up rownames for clarity

# Plot the coefficients at the optimal lambda
barplot(
  height = coef_df[-1, 1],             # Exclude intercept for plotting
  names.arg = coef_df[-1, "Variable"], # Variable names
  las = 2,                             # Rotate labels for better readability
  main = paste("Coefficients at Optimal Lambda =", round(opt_lambda, 2)),
  xlab = "Predictors",
  ylab = "Coefficient Values",
  col = "skyblue",
  cex.names = 0.8                      # Adjust label size
)

```

```{r}
# Check if glmnet standardized the data
print(ridge_model$beta) # Standardized coefficients for each lambda

# Retrieve optimal lambda
print(opt_lambda)

# Recalculate Ridge model if needed with standardized = FALSE (to test)
ridge_model_no_standard <- glmnet(predictors, response, alpha = 0, standardize = FALSE)

```

# Results

## Comparison of Outputs

## Deviations

## Interpreting the Results

# Challenges and Insights

## Challenges

## Insights

# Conclusion

## Significance

## Summary

## Future Work

## Acknowledgements

# References
