---
title: "Replication and Extension: Instabilities of Regression Estimates Relating Air Pollution to Mortality"
subtitle: "STAT 206 Project"
author: "Sharvee Joshi, Makena Grigsby, Wonkeun Lee, Behzad FallahiFard"
date: "today"
echo: true
format:
  html:
    toc: true
    toc-location: left
    embed-resources: true
    code-line-numbers: true
  pdf:
    code-line-numbers: true
bibliography: stat206project.bib
---

# Introduction

Air pollution is not only an environmental concern, but also a public health one. There is substantial evidence that indicates a link in adverse health outcomes, including an increase in mortality rates. Using quantitative analysis, statisticians in the past have been able to determine the exact link as well as use a multitude of methods to study these results.

One example is seen in the article titled: *"Instabilities of Regression Estimates Relating Air Pollution to Mortality"*. The article aimed to explore the relationship between air pollution and mortality rates with the use of regression-based estimates. The article authors use a various number of factors, such as rain, temperature, different levels of toxic chemicals, to determine a linear model between air pollution and mortality rates. They used ridge regression to create better coefficient estimates due to the underlying issues the data faced.

## Objective

The purpose of this report is to replicate the analyses that were conducted in the original study, using techniques that we have discussed in this class, as well as the use of modern technology. An important note is that the article was written in the 1970s, and the data and statistical methods were most likely conducted during that time. We are aware of the advanced technology we have now, and we hope to compare our output versus the original output, and perhaps observe better results. We also hope to learn more about statistical modeling as well as contribute to a better understanding of how methodologies change over time. Finally, we hope to truly understand and do our best to interpret the relationship between air pollution and mortality rates.

# Literature Review

# Methodology From Author

## Data Description

## Regression Models

## Assumptions

## Reproduction Plan

# Implementation

## Exploratory Data Analysis

```{r}
# pulling the data
library("bestglm")
data <- mcdonald

head(data)
```
```{r}
#Distribution of respose variable
#install.packages("ggplot2")
library(ggplot2)
ggplot(data, aes(x = MORT)) +
  geom_histogram(binwidth = 50, fill = "skyblue", color = "black") +
  labs(
    title = "Distribution of Mortality Rates",
    x = "Mortality",
    y = "Frequency"
  ) +
  theme_minimal()
```



```{r}
# Scatterplot: Mortality vs. Pollution Variables
hc <- ggplot(data, aes(x = HC, y = MORT)) +
  geom_point(color = "blue") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(
    title = "Mortality vs. Hydrocarbon Pollution",
    x = "Hydrocarbon Pollution Potential",
    y = "Mortality"
  ) +
  theme_minimal()

nox <- ggplot(data, aes(x = NOX, y = MORT)) +
  geom_point(color = "blue") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(
    title = "Mortality vs. NoX Pollution",
    x = "Hydrocarbon Pollution Potential",
    y = "Mortality"
  ) +
  theme_minimal()

so2 <- ggplot(data, aes(x = SOx, y = MORT)) +
  geom_point(color = "blue") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(
    title = "Mortality vs. SO2 Pollution",
    x = "Hydrocarbon Pollution Potential",
    y = "Mortality"
  ) +
  theme_minimal()

library(ggpubr)
ggarrange(
  hc,nox,so2
)
```
```{r}
# correlation heat map
library(reshape2)

# Calculate correlation matrix
cor_matrix <- cor(data)
cor_data <- melt(cor_matrix)

# Heatmap
ggplot(cor_data, aes(Var1, Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white",
    midpoint = 0, limit = c(-1, 1)
  ) +
  labs(title = "Correlation Heatmap", x = "", y = "") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```




## Multiple Linear Regression
```{r}
#first full model
ols_model <- lm(MORT ~., data = data)
summary(ols_model)
```
```{r}
ggplot(data = data.frame(
  residuals = residuals(ols_model),
  fitted = fitted(ols_model)
), aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Residuals vs. Fitted Values",
    x = "Fitted Values",
    y = "Residuals"
  ) +
  theme_minimal()
```


### Mallow's $C_p$

```{r}
#plotting mallow's Cp

library(ggplot2)  # For plotting
library(leaps)    # For all subset regression

# str(data)

```


```{r}
#subset the modeling using stepAIC, or the Akaike Information Criterion
library(MASS)
ols_aic <- stepAIC(ols_model, direction = "both", trace = FALSE)
summary(ols_aic)
```
```{r}
# get the AIC for Cp Mallows then compare with stepAIC, full, and CP Mallows

```
Mallow's C_p performs slightly better 

## Ridge Regression

## Principal Component Regression (PCR)

### Motivation for PCR

In a linear regression model, when the predictors ($X_i$s) are linearly dependent or highly correlated, we say that multicollinearity exists in the model, which causes the following issue for the fitted multiple regression model:

-   The estimate of any parameter, say $\hat{\beta_2}$, depends on all the variables included in the model. For example, $\hat{\beta_2}$ does not merely reflect the effect of the $X_2$ variable on the response variable $Y$. Due to multicollinearity, the other variables affect the estimate of the parameter $\beta_2$. As a result, our inference about the effect of each predictor on the response is not reliable.

We have 15 predictors in our dataset, and we want to investigate the relationship between them and the response variable (mortality). There is a high correlation between most of the predictors. @fig-corr-xx illustrates this:

```{r}
#|warning: false
#------------------Cleaning the memory
rm(list=ls())
#------------------Reading Data
library(leaps)
library("bestglm")
data <- data.frame(mcdonald)
#-----------------Centralize-Normalize Data
data.norm <- data
for(i in 1:ncol(data)){
  data.norm[,i] <- data[,i]-mean(data[,i])
  data.norm[,i] <- data.norm[,i]/sqrt(sum((data.norm[,i])^2))
}
#----------------Separating X and Y(Mort)
x.norm <- data.norm
mort.norm <- x.norm$MORT
x.norm$MORT <- NULL
#----------------Correlation Plot for predictors (X)
res_xx <- cor(x.norm, method="pearson")
library(corrplot)
```

```{r}
#| label: fig-corr-xx
#| fig-cap: Correlation plot for 15 predictors of the data set
corrplot(res_xx, method = 'ellipse', order = 'AOE', type = 'upper')
```

The correlation plot of the predictors indicates there is Multicolleniarity among predictors. For overcoming this issue the authors of the paper used Ridge Regression method and after exploring and comparing several Ridge and Ordinary Least Square models (Multiple Regression Models) they conclude that:

::: callout
"In summary, the ridge regression coefficients obtained at $k = o.2$ for the fifteen explanatory variables listed in Table IV, with one notable exception($NO_x$ ), appear to be reasonable values upon which to base a quantification of the association between these variables and the total mortality rate."
:::

In the following, we present a Principal Component Regression model with all fifteen predictors as an alternative model for overcoming multicollinearity and compare it with the final Ridge Regression model suggested by the authors.

### Modeling PCR

In PCR, we try to provide a weighted linear combination of the predictors ($X_i$s) like: $$Z_k = v_{k1}X_1 + v_{k2}X_2 + \dots + v_{kp}X_p$$ We then replace the correlated variables $[X_1, X_2, \dots, X_p]$ with $Z = [Z_1, Z_2, \dots, Z_p]$ which are uncorrelated, so the regression model $y = Z\beta_z + \epsilon$ does not suffer from multicollinearity.

First, we need to centralize and normalize all the variables for PCR. In the following, we assume the $y$ and $X$ variables are all centralized and normalized and show how to find the $Z$ matrix.

$$\text{Correlated: $X=[X_1, X_2, \dots, X_p]$ $\xrightarrow{\text{PC Analysis}}$ Uncorrelated: $Z=[Z_1, Z_2, \dots, Z_p]$}$$

We call $Z_i$ the i-th **principal component** of matrix $X$.

In ordinary least squares,

$$\hat{\beta_x}=\left( X'X \right)^{-1}X'y$$

We perform PCA on matrix $X$ through singular value decomposition. We obtain $$UDV'=X$$ $U$ is the left singular vectors matrix, $D$ is a diagonal matrix consisting of non-negative singular values, and $V$ is the matrix of the right singular vectors matrix (eigenvectors matrix). We calculate matrix $$Z=XV$$

The columns of $Z$ are linearly independent. If we define

$$\hat{\beta_z}=\left( Z'Z \right)^{-1}Z'y$$ then:

$$\hat{y}=X\hat{\beta_x}=Z\hat{\beta_z}$$ So, We have the regression model $y = Z\beta_z + \epsilon$ without multilinearity, but gives the same predictions of the intial model $y = X\beta_x + \epsilon$.

### Principal component Matrix $Z$, for our data.

```{r}
x.pca1 <- prcomp(x.norm)
z <- x.pca1$x
head(z)
```

### Principal components are uncorrolated:

```{r}
#| label: fig-cor-pc
#| fig-cap: Correlation plot for 15 Principal Components (Columns of the matrix Z)
res_zz <- cor(z, method="pearson")
library(corrplot)
corrplot(res_zz, method = 'ellipse', order = 'AOE', type = 'upper')
```

One question is: after converting the matrix 
$X$ to $Z$, how much of the information (variance) from the original matrix $X$ does the principal component matrix $Z$ retain?

```{r}
variation <- 100*cbind(x.pca1$sdev/sum(x.pca1$sdev),rep(0    ,15),cumsum(x.pca1$sdev)/sum(x.pca1$sdev))
colnames(variation) <- c("Var(%)","   ","CumVar(%)")
rownames(variation) <- c("PC1","PC2","PC3","PC4","PC5","PC6","PC7","PC8","PC9","PC10","PC11","PC12","PC13","PC14","PC15")
round(variation[,c(1,3)],1)
```

The first principal component ($PC_1$) only accounts $27$% of the variation (information) of $X$ matrix, the first two principal components ($PC_1$ and $PC_2$) account $30$% of the $X$ matrix variation and so on. All the principal components matrix,$Z=[PC_1, PC_2,\dots,PC_{15}]$, account $100$% of the variation of of the $X$ matrix.

```{r}
#| label: fig-scree
#| fig-cap: The scree plot show the cumulative variance explained by principal components.
p <- ncol(x.norm)
PVE <- rep(NA,p)
for(i in 1:15){ PVE[i]<- variation[i,3] }
barplot( PVE, names.arg = 1:p, main = "scree plot", 
         xlab = "number of PCs", 
         ylab = "% of variance explained" )
```

### Full PCR Model

In the following we have fitted the Principal Component Regression model ($\hat{y} = Z\hat{\beta_z}$) for our data.

```{r}
data_pcr <- data.frame(cbind(z,mort.norm))
full_pcr <- lm(mort.norm ~.,data=data_pcr)
summary(full_pcr)
```

This model has exactly the same prediction values, residual sum of squares, and $R^2$ as the Ordinary Least Squares model with the 15 predictors. However, there are some insignificant coefficients in this model. By following [@christensen2019statistical], we can drop the predictors corresponding to insignificant coefficients and build the PCR based on the predictors with significant effects.

### Reduced PCR Model

The principal components with significant effects in the full model are:

$$PC_1, PC_3, PC_6, PC_7, PC_9, PC_{12}$$ These subsets of the principal components account for `{r} round(sum(variation[c(1,3,6,7,9,12),1]))`% of the variation in the original predictors ($X_i$s). We may say that the other 49% of the variation is not significantly related to the response variable.

So the reduced model is:

```{r}
reduced_pcr <- lm(mort.norm ~PC1+PC3+PC6+PC7+PC9+PC12,data=data_pcr)
summary(reduced_pcr)
```

$$\hat{MORT}= 0.251PC_1-0.319PC_3+0.339PC_6 -0.279PC_7+0.3PC_9+0.470PC_{12} $$ We know each PC is a linear combination of all the original predictors, as follow:

```{r}
v<- x.pca1$rotation[,c(1,3,6,7,9,12)]
v
```

### Model Assumption for PCR

```{r}
#| label: fig-residual-ey
#| fig-cap: Residual versus fitted values of reduced PCR model
plot(reduced_pcr$fitted.values,reduced_pcr$residuals, xlab = "fitted values for reduced PCR model", ylab = "residuals of the reduced PCR model")
abline(h=0,lty=2,col="blue")
```

```{r}
#| label: fig-hist-pcr
#| fig-cap: Histogram for the residuals of the reduced PCR model
hist(reduced_pcr$residuals,breaks=13,xlab = "residuals of the reduced PCR model")
```

```{r}
#| label: fig-qq-pcr
#| fig-cap: Normal Q-Q plot for the residuals of the reduced PCR model
qqnorm(reduced_pcr$residuals)
qqline(reduced_pcr$residuals)
shapiro.test(reduced_pcr$residuals)
```


It seems the residuals of the reduced PCR model does not violate the assumptions.

### Final PCR Model in Terms of the Original Predictors

The reduced model meets all the model assumptions and we consider it as the final PCR model.

we can also rewrite the model in terms of the original predictors:

$$\hat{\beta_x}=V\hat{\beta_z}$$ where V is the vector of selected eigenvectors corresponding to selected principal components($[PC_1, PC_3, PC_6, PC_7, PC_9, PC_{12}]$)

```{r}
beta_x <- as.matrix(v)%*%as.matrix(reduced_pcr$coefficients)[-1,]
beta_x
```

$$
\begin{align}
\hat{MORT}&=0.224(PREC)-0.275(JANT)-0.134(JULT)+0.032(OVR65)-0.008(POPN)\\
&-0.350(EDUC)-0.071(HOUS)+0.129(DENS)+0.574(NONW)+0.574(WWDRK)\\
&+0.014(POOR)+0.003(HC)+0.012(NO_x)+0149(SO_2)+0.085(HUMID)
\end{align}
$$

For this model $R^2=0.712$ which is 14% larger (better) than the corresponding Ridge model (with k=0.2). In terms of residual sum of squared, the ridge model slightly works better than PCR. The residual sum of squared for PCR model is $\phi^*=0.287$ , which is slightly larger than the residual sum of squares for the Ridge model. The coefficients estimated for predictors are not much different from the corresponding ones in the Ridge model, except for OVR65, WWDRK, and HC, which have different signs in the two models. For OVR65 and HC, the difference in sign may not be a concern, as the estimated coefficients for these two predictors in both models are close to zero and do not significantly affect the models. However, for WWDRK, the PCR model considers that WWDRK has a positive moderate effect on mortality, while the Ridge model considers a negligible effect of -0.034.

# Results

## Comparison of Outputs

```{r}
# want to compare the AIC, ridge regression, CP Mallow, and PCA to the full model 
```


## Deviations

## Interpreting the Results

# Challenges and Insights

## Challenges

## Insights

# Conclusion

## Significance

## Summary

## Future Work

## Acknowledgements

# References
