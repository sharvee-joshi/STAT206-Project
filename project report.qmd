# Introduction

Air pollution is not only an environmental concern, but also a public health one. There is substantial evidence that indicates a link in adverse health outcomes, including an increase in mortality rates. Using quantitative analysis, statisticians in the past have been able to determine the exact link as well as use a multitude of methods to study these results.

One example is seen in the article titled: *"Instabilities of Regression Estimates Relating Air Pollution to Mortality"*. The article aimed to explore the relationship between air pollution and mortality rates with the use of regression-based estimates. The authors use a various number of factors, such as rain, temperature, different levels of toxic chemicals, to determine a linear model between air pollution and mortality rates. They used ridge regression to create better coefficient estimates due to the underlying issues the data faced.

## Objective

The purpose of this report is to replicate the analyses that were conducted in the original study, using techniques that we have discussed in this class, as well as the use of modern technology. An important note is that the article was written in the 1970s, and the data and statistical methods were most likely conducted during that time. We are aware of the advanced technology we have now, and we hope to compare our output versus the original output, and perhaps observe better results. We also hope to learn more about statistical modeling as well as contribute to a better understanding of how methodologies change over time. Finally, we hope to truly understand and do our best to interpret the relationship between air pollution and mortality rates.

# Literature Review

# Methodology

## Data Description

## Regression Models

## Assumptions

## Reproduction Plan

# Implementation

## Exploratory Data Analysis

```{r}

# pulling the data
library("bestglm")
data <- mcdonald

head(data)
```

## Multiple Linear Regression

## Ridge Regression


```{r}
library(glmnet)
library(MASS)

fit.ols <- lm(MORT ~ ., data = mcdonald)
summary(fit.ols)

```

```{r}
plot(fit.ols)
plot(fit.ridge)
```

Compile and make it efficient
Step 1
```{r}
#load data
response <- mcdonald$MORT
predictors <- as.matrix(mcdonald[, -which(names(mcdonald) == "MORT")])
```

Step 2: Optimal Lambda
```{r}
# Cross-validation for ridge regression
ridge_cv <- cv.glmnet(predictors, response, alpha = 0)
opt_lambda <- ridge_cv$lambda.min
cat("Optimal Lambda:", opt_lambda, "\n")

```

Step 3: Fit Ridge Model 
```{r}
ridge_model <- glmnet(predictors, response, alpha = 0, lambda = opt_lambda)

# Extract coefficients
ridge_coefs <- as.data.frame(as.matrix(coef(ridge_model)))
ridge_coefs$Variable <- rownames(ridge_coefs)
colnames(ridge_coefs) <- c("Coefficients", "Variable")
ridge_coefs <- ridge_coefs[order(abs(ridge_coefs$Coefficients), decreasing = TRUE), ]  # Optional: Sort by magnitude
print(ridge_coefs)

```

Step 4: Compare OLS and Ridge Coefficients

```{r}
ols_model <- lm(MORT ~ ., data = mcdonald)
ols_coefs <- as.data.frame(coef(ols_model))
colnames(ols_coefs) <- "OLS_Coefficients"
ols_coefs$Variable <- rownames(ols_coefs)

# Merge 
coef_comparison <- merge(ols_coefs, ridge_coefs, by = "Variable", all = TRUE)
print(coef_comparison)
```

Step 5: Multicollinearity Analysis

```{r}
library(magrittr)  # Ensure magrittr is loaded

# Compute VIF for OLS
vif_values <- car::vif(ols_model)
vif_df <- data.frame(Variable = names(vif_values), VIF = vif_values)

# Optional: Compute approximate VIF for ridge
ridge_vif <- function(predictors, lambda) {
  diag(solve(t(predictors) %*% predictors + diag(lambda, ncol(predictors)))) %>% sum()
}

ridge_vif_value <- ridge_vif(predictors, opt_lambda)
cat("Approximate Ridge VIF:", ridge_vif_value, "\n")

# Compare VIF values
print(vif_df)

```

Step 6: Correlation Heatmaps

```{r}
# Compute correlation of original predictors (OLS)
ols_cor <- cor(predictors)
heatmap(ols_cor, symm = TRUE, main = "OLS Correlation Heatmap")

# Compute Ridge-transformed predictors
ridge_coefs <- as.matrix(coef(ridge_model, s = opt_lambda))[-1, , drop = FALSE]  # Exclude intercept
ridge_transformed <- scale(predictors) %*% ridge_coefs

# Ensure ridge_transformed is a matrix with multiple columns
if (ncol(ridge_transformed) < 2) {
  stop("Ridge transformation resulted in insufficient variables for correlation matrix.")
}

# Compute correlation of Ridge-transformed predictors
ridge_cor <- cor(ridge_transformed)
heatmap(ridge_cor, symm = TRUE, main = "Ridge Correlation Heatmap")


```

Step 7: Predictive Accuracy 
```{r}
# OLS predictions and RMSE
ols_preds <- predict(ols_model, newdata = as.data.frame(predictors))
ols_rmse <- sqrt(mean((ols_preds - response)^2))

# Ridge predictions and RMSE
ridge_preds <- predict(ridge_model, newx = predictors, s = opt_lambda)
ridge_rmse <- sqrt(mean((ridge_preds - response)^2))

# Display results
rmse_df <- data.frame(Model = c("OLS", "Ridge"), RMSE = c(ols_rmse, ridge_rmse))
print(rmse_df)

plot(ols_preds)
plot(ols_rmse)

plot(ridge_cv)
```

Step 8: Ridge Trace Plot

```{r}
ridge_model <- glmnet(predictors, response, alpha = 0)

coef_matrix <- as.matrix(coef(ridge_model))
print(dim(coef_matrix))  # Check that you now have multiple columns

matplot(
  log(ridge_model$lambda), t(coef_matrix[-1, ]), 
  type = "l", lty = 1, col = 1:nrow(coef_matrix),
  xlab = "Log(Lambda)", ylab = "Coefficients",
  main = "Ridge Trace Plot"
)
```
```{r}
# Extract coefficients for the optimal lambda
opt_lambda_index <- which.min(abs(ridge_model$lambda - opt_lambda))  # Find closest lambda index
coef_opt_lambda <- coef(ridge_model, s = opt_lambda)                # Extract coefficients

# Convert to a data frame for easier manipulation
coef_df <- as.data.frame(as.matrix(coef_opt_lambda))
coef_df$Variable <- rownames(coef_opt_lambda)
rownames(coef_df) <- NULL  # Clean up rownames for clarity

# Plot the coefficients at the optimal lambda
barplot(
  height = coef_df[-1, 1],             # Exclude intercept for plotting
  names.arg = coef_df[-1, "Variable"], # Variable names
  las = 2,                             # Rotate labels for better readability
  main = paste("Coefficients at Optimal Lambda =", round(opt_lambda, 2)),
  xlab = "Predictors",
  ylab = "Coefficient Values",
  col = "skyblue",
  cex.names = 0.8                      # Adjust label size
)

```

# Results

## Comparison of Outputs

## Deviations

## Interpreting the Results

# Challenges and Insights

## Challenges

## Insights

# Conclusion

## Significance

## Summary

## Future Work

## Acknowledgements

# References
