---
title: "Replication and Extension: Instabilities of Regression Estimates Relating Air Pollution to Mortality"
subtitle: "STAT 206 Project"
author: "Sharvee Joshi, Makena Grigsby, Wonkeun Lee, Behzad FallahiFard"
date: "today"
echo: true
format:
  html:
    toc: true
    toc-location: left
    embed-resources: true
    code-line-numbers: true
  pdf:
    code-line-numbers: true
bibliography: stat206project.bib
---

# Introduction

Air pollution is not only an environmental concern, but also a public health one. There is substantial evidence that indicates a link in adverse health outcomes, including an increase in mortality rates. Using quantitative analysis, statisticians in the past have been able to determine the exact link as well as use a multitude of methods to study these results.

One example is seen in the article titled: *"Instabilities of Regression Estimates Relating Air Pollution to Mortality"*. The article aimed to explore the relationship between air pollution and mortality rates with the use of regression-based estimates. The article authors use a various number of factors, such as rain, temperature, different levels of toxic chemicals, to determine a linear model between air pollution and mortality rates. They used ridge regression to create better coefficient estimates due to the underlying issues the data faced.

## Objective

The purpose of this report is to replicate the analyses that were conducted in the original study, using techniques that we have discussed in this class, as well as the use of modern technology. An important note is that the article was written in the 1970s, and the data and statistical methods were most likely conducted during that time. We are aware of the advanced technology we have now, and we hope to compare our output versus the original output, and perhaps observe better results. We also hope to learn more about statistical modeling as well as contribute to a better understanding of how methodologies change over time. Finally, we hope to truly understand and do our best to interpret the relationship between air pollution and mortality rates.


# Methodology From Author

In the article, *"Instabilities of Regression Estimates Relating Air Pollution to Mortality"*, the author uses multiple linear regression, or the ordinary least squares, as their initial model. The initial, or full model presented was:
$$
Y = X \underline{\beta} + \underline{\epsilon}
$$
where $Y$ represents the response variable which is the mortality rate collected between 1959 to 1961. $X$ represents a matrix of predictors, and $\underline{\beta}$ are the regression coefficients from the full model. 

The full model was then used to estimate the association between the mortality rates and given predictors, as well as identify any instabilities in the coefficients caused by high multicollinearity. A common pattern seen in this paper is the high multicollinearity found between the predictors, especially in hydrocarbon, nitrogen oxides, and sulfer dioxides. These gases in particular happened to be highly correlated with one another. 

In order to address the multicollinearity, the author turns to two different methods: Ridge Regression and Variable Selection via Mallows' $C_p$ Criterion. The author first uses ridge regression by adding a penalty term to the Ordinary Least Squares objective function that looks like:
$$
\beta^* = argmin(||Y - X\beta||^2 + k||\beta||^2)
$$
In this case, we see the penalty term, or $k||\beta||^2$, will shrink the coefficients, which will result in a reduction of variance at the trade off of introducing slight bias. However, this will also result in the estimates stabilizing. The author also uses ridge trace to visualize these changes as the penalty term increases.

Finally, the author goes onto to use Variable Selection to compare the reduced models by using Mallows' $C_p$ Criterion and the Ridge Trace Analysis. In order to compare all the models, the author then uses the residual sum of squares, the coefficient of determination, $R^2$, and the how stable the coefficients were via ridge trace analysis. 

## Data Description



## Assumptions

## Reproduction Plan

Our plan to recreate the author's intentions was simple: redo all of the models using the exact methods the author used, only using modern technology like R. We divided up the tasks fairly between the teammates, making sure each one of us was tasked with researching and implementing a different model. We chose to stick with the original use of Ridge Regression and Mallows' $C_p$ Criterion, however, we expanded to using Principle Component Analysis as well as the Akaike's Criterion to see if there would be any sort of deviations from the paper that could be explained. The next part of the report shows, in detail, the steps we took. 

# Implementation

## Exploratory Data Analysis

```{r}
# pulling the data
library("bestglm")
data <- mcdonald
head(data)
```

```{r}
# in order to replicate the paper, we need to centralize and standardize the data before we continue
data.norm <- data
for(i in 1:ncol(data)){
  data.norm[,i] <- data[,i]-mean(data[,i])
  data.norm[,i] <- data.norm[,i]/sqrt(sum((data.norm[,i])^2))
}
```



```{r}
#Distribution of response variable
#install.packages("ggplot2")
library(ggplot2)
ggplot(data, aes(x = MORT)) +
  geom_histogram(binwidth = 50, fill = "skyblue", color = "black") +
  labs(
    title = "Distribution of Mortality Rates",
    x = "Mortality",
    y = "Frequency"
  ) +
  theme_minimal()
```



```{r}
# Scatterplot: Mortality vs. Pollution Variables
hc <- ggplot(data, aes(x = HC, y = MORT)) +
  geom_point(color = "blue") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(
    title = "Mortality vs. Hydrocarbon Pollution",
    x = "Hydrocarbon Pollution Potential",
    y = "Mortality"
  ) +
  theme_minimal()

nox <- ggplot(data, aes(x = NOX, y = MORT)) +
  geom_point(color = "blue") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(
    title = "Mortality vs. NoX Pollution",
    x = "Hydrocarbon Pollution Potential",
    y = "Mortality"
  ) +
  theme_minimal()

so2 <- ggplot(data, aes(x = SOx, y = MORT)) +
  geom_point(color = "blue") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(
    title = "Mortality vs. SO2 Pollution",
    x = "Hydrocarbon Pollution Potential",
    y = "Mortality"
  ) +
  theme_minimal()

library(ggpubr)
ggarrange(
  hc,nox,so2
)
```



## The Full Model
```{r}
#first full model
ols_model <- lm(MORT ~., data = data.norm)
summary(ols_model)
```
```{r}
ggplot(data = data.frame(
  residuals = residuals(ols_model),
  fitted = fitted(ols_model)
), aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Residuals vs. Fitted Values",
    x = "Fitted Values",
    y = "Residuals"
  ) +
  theme_minimal()
```

## Ridge Regression


## Variable Selection

### Mallow's $C_p$

```{r}
#plotting mallow's Cp

library(ggplot2)  # For plotting
library(leaps)    # For all subset regression

#separating the data for response and predictors
response <- data.norm$MORT
predictors <- names(data.norm)[names(data.norm) != "MORT"]

#Step 1: use subsets regression to perform cp mallows calculations
subs_reg <- regsubsets(response ~ ., data = data.norm, nvmax = length(predictors))
subsets_summary <- summary(subs_reg)

#Step 2: get mallows cp and determined number of preds
cp_dat <- data.frame(num_pred = 1:length(subsets_summary$cp) - 1, cp = subsets_summary$cp)

#Step 3: create two new variables: the number of parameters plus the intercept, and then the difference between the cp value and the parameter variable. We want to pick the model that will have the number of predictors be very close to the CP value
cp_dat$parameters <- cp_dat$num_pred + 1 # Include the intercept
cp_dat$cp_diff <- abs(cp_dat$cp - cp_dat$parameters) # Difference between Cp and p
best_cp_row <- cp_dat[which.min(cp_dat$cp_diff), ] # Row with the best match

# Step 4: Visualize Mallows' Cp
ggplot(cp_dat, aes(x = num_pred, y = cp)) +
  geom_point() +  # Plot all points
  geom_line() +   # Connect points with a line
  geom_point(data = best_cp_row, aes(x = num_pred, y = cp),
             color = "red", size = 3) +  # Highlight the desired point
  labs(
    title = paste("Mallows' Cp Plot (Highlighting 15 Predictors"),
    x = "Number of Predictors",
    y = "Mallows' Cp"
  ) +
  theme_minimal()


```
```{r}
library(knitr)

# Step 3: Sort by Difference
cp_table <- cp_dat[order(cp_dat$cp_diff), ] # Sort by smallest difference

# Step 4: Display Table
kable(cp_table, col.names = c(
  "Number of Predictors", "Mallows' Cp", "Parameters (p+1)", "Difference (|Cp - Parameters|)"
), caption = "Mallows' Cp Table with Differences", format = "html") # Adjust format if not in markdown

```


```{r}
#Step 1: find out the optimal number of coefficients according to CP, then use that to create a variable for the predictors
min_cp_index <- which.min(cp_dat$cp_diff)
best_predictors <- names(coef(subs_reg, id = min_cp_index))
best_predictors <- best_predictors[best_predictors != "(Intercept)"]

# Step 2: create our reduced model formula using the above predictors
red_cp_formula <- as.formula(paste("MORT ~", paste(best_predictors, collapse = " + ")))

# Step 3: fit reduced model using normalized data and spit out summary
red_cp_model <- lm(red_cp_formula, data = data.norm)
summary(red_cp_model)
```
```{r}
#checking the residuals for the reduced CP Model
ggplot(data = data.frame(
  residuals = residuals(red_cp_model),
  fitted = fitted(red_cp_model)
), aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Residuals vs. Fitted Values",
    x = "Fitted Values",
    y = "Residuals"
  ) +
  theme_minimal()
```
```{r}
qqnorm(red_cp_model$residuals)
qqline(red_cp_model$residuals)

shapiro.test(red_cp_model$residuals)
```
Analysis here



```{r}
#compare to full model - OLS model

cat("AIC of full model:", AIC(ols_model), "\n")
cat("AIC of reduced model using Mallow's:", AIC(red_cp_model), "\n")

```


### Akaike's Information Criterion

```{r}
#subset the modeling using stepAIC, or the Akaike Information Criterion
library(MASS)
ols_aic <- stepAIC(ols_model, direction = "both", trace = FALSE)
summary(ols_aic)
```

## Principal Component Regression (PCR)

### Motivation for PCR

In a linear regression model, when the predictors ($X_i$s) are linearly dependent or highly correlated, we say that multicollinearity exists in the model, which causes the following issue for the fitted multiple regression model:

-   The estimate of any parameter, say $\hat{\beta_2}$, depends on all the variables included in the model. For example, $\hat{\beta_2}$ does not merely reflect the effect of the $X_2$ variable on the response variable $Y$. Due to multicollinearity, the other variables affect the estimate of the parameter $\beta_2$. As a result, our inference about the effect of each predictor on the response is not reliable.

We have 15 predictors in our dataset, and we want to investigate the relationship between them and the response variable (mortality). There is a high correlation between most of the predictors. @fig-corr-xx illustrates this:

```{r}
#|warning: false
#------------------Cleaning the memory
rm(list=ls())
#------------------Reading Data
library(leaps)
library("bestglm")
data <- data.frame(mcdonald)
#-----------------Centralize-Normalize Data
data.norm <- data
for(i in 1:ncol(data)){
  data.norm[,i] <- data[,i]-mean(data[,i])
  data.norm[,i] <- data.norm[,i]/sqrt(sum((data.norm[,i])^2))
}
#----------------Separating X and Y(Mort)
x.norm <- data.norm
mort.norm <- x.norm$MORT
x.norm$MORT <- NULL
#----------------Correlation Plot for predictors (X)
res_xx <- cor(x.norm, method="pearson")
library(corrplot)
```

```{r}
#| label: fig-corr-xx
#| fig-cap: Correlation plot for 15 predictors of the data set
corrplot(res_xx, method = 'ellipse', order = 'AOE', type = 'upper')
```

The correlation plot of the predictors indicates there is Multicolleniarity among predictors. For overcoming this issue the authors of the paper used Ridge Regression method and after exploring and comparing several Ridge and Ordinary Least Square models (Multiple Regression Models) they conclude that:

::: callout
"In summary, the ridge regression coefficients obtained at $k = o.2$ for the fifteen explanatory variables listed in Table IV, with one notable exception($NO_x$ ), appear to be reasonable values upon which to base a quantification of the association between these variables and the total mortality rate."
:::

In the following, we present a Principal Component Regression model with all fifteen predictors as an alternative model for overcoming multicollinearity and compare it with the final Ridge Regression model suggested by the authors.

### Modeling PCR

In PCR, we try to provide a weighted linear combination of the predictors ($X_i$s) like: $$Z_k = v_{k1}X_1 + v_{k2}X_2 + \dots + v_{kp}X_p$$ We then replace the correlated variables $[X_1, X_2, \dots, X_p]$ with $Z = [Z_1, Z_2, \dots, Z_p]$ which are uncorrelated, so the regression model $y = Z\beta_z + \epsilon$ does not suffer from multicollinearity.

First, we need to centralize and normalize all the variables for PCR. In the following, we assume the $y$ and $X$ variables are all centralized and normalized and show how to find the $Z$ matrix.

$$\text{Correlated: $X=[X_1, X_2, \dots, X_p]$ $\xrightarrow{\text{PC Analysis}}$ Uncorrelated: $Z=[Z_1, Z_2, \dots, Z_p]$}$$

We call $Z_i$ the i-th **principal component** of matrix $X$.

In ordinary least squares,

$$\hat{\beta_x}=\left( X'X \right)^{-1}X'y$$

We perform PCA on matrix $X$ through singular value decomposition. We obtain $$UDV'=X$$ $U$ is the left singular vectors matrix, $D$ is a diagonal matrix consisting of non-negative singular values, and $V$ is the matrix of the right singular vectors matrix (eigenvectors matrix). We calculate matrix $$Z=XV$$

The columns of $Z$ are linearly independent. If we define

$$\hat{\beta_z}=\left( Z'Z \right)^{-1}Z'y$$ then:

$$\hat{y}=X\hat{\beta_x}=Z\hat{\beta_z}$$ So, We have the regression model $y = Z\beta_z + \epsilon$ without multilinearity, but gives the same predictions of the intial model $y = X\beta_x + \epsilon$.

### Principal component Matrix $Z$, for our data.

```{r}
x.pca1 <- prcomp(x.norm)
z <- x.pca1$x
head(z)
```

### Principal components are uncorrolated:

```{r}
#| label: fig-cor-pc
#| fig-cap: Correlation plot for 15 Principal Components (Columns of the matrix Z)
res_zz <- cor(z, method="pearson")
library(corrplot)
corrplot(res_zz, method = 'ellipse', order = 'AOE', type = 'upper')
```

One question is: after converting the matrix 
$X$ to $Z$, how much of the information (variance) from the original matrix $X$ does the principal component matrix $Z$ retain?

```{r}
variation <- 100*cbind(x.pca1$sdev/sum(x.pca1$sdev),rep(0    ,15),cumsum(x.pca1$sdev)/sum(x.pca1$sdev))
colnames(variation) <- c("Var(%)","   ","CumVar(%)")
rownames(variation) <- c("PC1","PC2","PC3","PC4","PC5","PC6","PC7","PC8","PC9","PC10","PC11","PC12","PC13","PC14","PC15")
round(variation[,c(1,3)],1)
```

The first principal component ($PC_1$) only accounts $27$% of the variation (information) of $X$ matrix, the first two principal components ($PC_1$ and $PC_2$) account $30$% of the $X$ matrix variation and so on. All the principal components matrix,$Z=[PC_1, PC_2,\dots,PC_{15}]$, account $100$% of the variation of of the $X$ matrix.

```{r}
#| label: fig-scree
#| fig-cap: The scree plot show the cumulative variance explained by principal components.
p <- ncol(x.norm)
PVE <- rep(NA,p)
for(i in 1:15){ PVE[i]<- variation[i,3] }
barplot( PVE, names.arg = 1:p, main = "scree plot", 
         xlab = "number of PCs", 
         ylab = "% of variance explained" )
```

### Full PCR Model

In the following we have fitted the Principal Component Regression model ($\hat{y} = Z\hat{\beta_z}$) for our data.

```{r}
data_pcr <- data.frame(cbind(z,mort.norm))
full_pcr <- lm(mort.norm ~.,data=data_pcr)
summary(full_pcr)
```

This model has exactly the same prediction values, residual sum of squares, and $R^2$ as the Ordinary Least Squares model with the 15 predictors. However, there are some insignificant coefficients in this model. By following [@christensen2019statistical], we can drop the predictors corresponding to insignificant coefficients and build the PCR based on the predictors with significant effects.

### Reduced PCR Model

The principal components with significant effects in the full model are:

$$PC_1, PC_3, PC_6, PC_7, PC_9, PC_{12}$$ These subsets of the principal components account for `{r} round(sum(variation[c(1,3,6,7,9,12),1]))`% of the variation in the original predictors ($X_i$s). We may say that the other 49% of the variation is not significantly related to the response variable.

So the reduced model is:

```{r}
reduced_pcr <- lm(mort.norm ~PC1+PC3+PC6+PC7+PC9+PC12,data=data_pcr)
summary(reduced_pcr)
```

$$\hat{MORT}= 0.251PC_1-0.319PC_3+0.339PC_6 -0.279PC_7+0.3PC_9+0.470PC_{12} $$ We know each PC is a linear combination of all the original predictors, as follow:

```{r}
v<- x.pca1$rotation[,c(1,3,6,7,9,12)]
v
```

### Model Assumption for PCR

```{r}
#| label: fig-residual-ey
#| fig-cap: Residual versus fitted values of reduced PCR model
plot(reduced_pcr$fitted.values,reduced_pcr$residuals, xlab = "fitted values for reduced PCR model", ylab = "residuals of the reduced PCR model")
abline(h=0,lty=2,col="blue")
```

```{r}
#| label: fig-hist-pcr
#| fig-cap: Histogram for the residuals of the reduced PCR model
hist(reduced_pcr$residuals,breaks=13,xlab = "residuals of the reduced PCR model")
```

```{r}
#| label: fig-qq-pcr
#| fig-cap: Normal Q-Q plot for the residuals of the reduced PCR model
qqnorm(reduced_pcr$residuals)
qqline(reduced_pcr$residuals)
shapiro.test(reduced_pcr$residuals)
```


It seems the residuals of the reduced PCR model does not violate the assumptions.

### Final PCR Model in Terms of the Original Predictors

The reduced model meets all the model assumptions and we consider it as the final PCR model.

we can also rewrite the model in terms of the original predictors:

$$\hat{\beta_x}=V\hat{\beta_z}$$ where V is the vector of selected eigenvectors corresponding to selected principal components($[PC_1, PC_3, PC_6, PC_7, PC_9, PC_{12}]$)

```{r}
beta_x <- as.matrix(v)%*%as.matrix(reduced_pcr$coefficients)[-1,]
beta_x
```

$$
\begin{align}
\hat{MORT}&=0.224(PREC)-0.275(JANT)-0.134(JULT)+0.032(OVR65)-0.008(POPN)\\
&-0.350(EDUC)-0.071(HOUS)+0.129(DENS)+0.574(NONW)+0.574(WWDRK)\\
&+0.014(POOR)+0.003(HC)+0.012(NO_x)+0149(SO_2)+0.085(HUMID)
\end{align}
$$

For this model $R^2=0.712$ which is 14% larger (better) than the corresponding Ridge model (with k=0.2). In terms of residual sum of squared, the ridge model slightly works better than PCR. The residual sum of squared for PCR model is $\phi^*=0.287$ , which is slightly larger than the residual sum of squares for the Ridge model. The coefficients estimated for predictors are not much different from the corresponding ones in the Ridge model, except for OVR65, WWDRK, and HC, which have different signs in the two models. For OVR65 and HC, the difference in sign may not be a concern, as the estimated coefficients for these two predictors in both models are close to zero and do not significantly affect the models. However, for WWDRK, the PCR model considers that WWDRK has a positive moderate effect on mortality, while the Ridge model considers a negligible effect of -0.034.

# Results

## Comparison of Outputs

```{r}
# want to compare the AIC, ridge regression, CP Mallow, and PCA to the full model 
```


## Deviations

## Interpreting the Results

# Challenges and Insights

## Challenges

## Insights

# Conclusion

## Significance

## Summary

## Future Work

## Acknowledgements

We'd like to first start off by thanking Dr. Landeros for the opportunity to recreate an article as profound as this one... add on here