---
title: "Replication and Extension: Instabilities of Regression Estimates Relating Air Pollution to Mortality"
subtitle: "STAT 206 Project"
author: "Sharvee Joshi, Makena Grigsby, Wonkeun Lee, Behzad FallahiFard"
date: "today"
echo: true
format:
  html:
    toc: true
    toc-location: left
    embed-resources: true
    code-line-numbers: true
  pdf:
    code-line-numbers: true
bibliography: stat206project.bib
---

# Introduction

Air pollution is not only an environmental concern, but also a public health one. There is substantial evidence that indicates a link in adverse health outcomes, including an increase in mortality rates. Using quantitative analysis, statisticians in the past have been able to determine the exact link as well as use a multitude of methods to study these results.

One example is seen in the article titled: *"Instabilities of Regression Estimates Relating Air Pollution to Mortality"*[@technometrics2]. The article aimed to explore the relationship between air pollution and mortality rates with the use of regression-based estimates. The article authors use a various number of factors, such as rain, temperature, different levels of toxic chemicals, to determine a linear model between air pollution and mortality rates. They used ridge regression to create better coefficient estimates due to the underlying issues the data faced.

## Objective

The purpose of this report is to replicate the analyses that were conducted in the original study, using techniques that we have discussed in this class, as well as the use of modern technology. An important note is that the article was written in the 1970s, and the data and statistical methods were most likely conducted during that time. We are aware of the advanced technology we have now, and we hope to compare our output versus the original output, and perhaps observe better results. We also hope to learn more about statistical modeling as well as contribute to a better understanding of how methodologies change over time. Finally, we hope to truly understand and do our best to interpret the relationship between air pollution and mortality rates.


# Methodology From Author

In the article, *"Instabilities of Regression Estimates Relating Air Pollution to Mortality"*, the author uses multiple linear regression, or the ordinary least squares, as their initial model. The initial, or full model presented was:
$$
Y = X \underline{\beta} + \underline{\epsilon}
$$
where $Y$ represents the response variable which is the mortality rate collected between 1959 to 1961. $X$ represents a matrix of predictors, and $\underline{\beta}$ are the regression coefficients from the full model. 

The full model was then used to estimate the association between the mortality rates and given predictors, as well as identify any instabilities in the coefficients caused by high multi-collinearity. A common pattern seen in this paper is the high multi-collinearity found between the predictors, especially in hydrocarbon, nitrogen oxides, and sulfur dioxides. These gases in particular happened to be highly correlated with one another. 

In order to address the multi-collinearity, the author turns to two different methods: Ridge Regression and Variable Selection via Mallows' $C_p$ Criterion. The author first uses ridge regression by adding a penalty term to the Ordinary Least Squares objective function that looks like:
$$
\beta^* = argmin(||Y - X\beta||^2 + k||\beta||^2)
$$
In this case, we see the penalty term, or $k||\beta||^2$, will shrink the coefficients, which will result in a reduction of variance at the trade off of introducing slight bias. However, this will also result in the estimates stabilizing. The author also uses ridge trace to visualize these changes as the penalty term increases.

Finally, the author goes onto to use Variable Selection to compare the reduced models by using Mallows' $C_p$ Criterion and the Ridge Trace Analysis. In order to compare all the models, the author then uses the residual sum of squares, the coefficient of determination, $R^2$, and the how stable the coefficients were via ridge trace analysis. 

## Data Description

The first thing we, as a group, had to do was standardize the data, much like the original authors. This is due to a multitude of reasons. The first reason was because all of the predictors had different scales. Variables like $NO_x$ levels had a different scale compared to variables like population density and education level. One variable was measured in parts per million where as another variable was measured by raw counts. Thus, it made sense to standardize the variables to make sure they were on the same scale, which would prevent any predictors with larger absolute values from dominating the regression model. 

The mortality data used in this analysis spans the years 1959 to 1961, covering age-adjusted mortality rates from 201 Standard Metropolitan Statistical Areas (SMSAs) in the United States. These data reflect mortality patterns during that specific period. In contrast, the pollution data was collected in 1963 and is based on pollution potential rather than direct measurements of pollutant concentrations. The pollution potential estimates were calculated using daily per-unit-area emissions combined with dispersion coefficients derived from factors such as mixing height, wind speed, and meteorological conditions.

It is worth noting that there is a 2- to 4-year time gap between the mortality data and the pollution data. While this temporal discrepancy may introduce uncertainty in establishing causal relationships, our team consulted with Professor Landeros to address this issue. It was concluded that air pollution levels during this period likely did not fluctuate significantly, minimizing the potential impact of this time gap. Furthermore, as the primary objective of this study is to replicate and extend the statistical methodologies of the original study rather than establish causation, we determined that this temporal gap would not compromise the validity of our analysis.

Standardizing the data also allowed for an improvement in performance of variable selection. One thing we did differently from the author was used the standardized data on Mallows' $C_p$.

::: callout
"In our computations, we used as input the raw data, i.e. our regression model for this part of the analysis was not standardized, and so a constant term is counted as a parameter"
:::

The author does not exactly explain the thought process behind this, but better results were seen when using the standardized data on Mallows' $C_p$.

By standardizing the data, we were also able to slightly reduce multi-collinearity. While this is a major problem we face with the data, standardizing it helped mitigate the problem.

Finally, we standardized the data, simply because that's what the author did. We are trying our best to first recreate the analysis the author did, hence, it only made sense to follow his outline, and then diverge with our own results. 

## Assumptions

1.	Temporal Stability of Pollution Levels
We assume that pollution levels between 1959–1961 (mortality data) and 1963 (pollution data) remained relatively stable. Despite the 2–4 year gap, this temporal discrepancy was deemed negligible for replicating the original study’s methodologies rather than establishing causation.

2.	Replication and Modern Extensions
The study replicated the original methodologies (OLS, Ridge Regression, Mallows’ C_p) while incorporating Principal Component Analysis (PCA) as a modern extension to address multicollinearity.

3.	Effectiveness of Multicollinearity Solutions
Ridge Regression, Mallows’ C_p, and PCA were assumed to effectively mitigate multicollinearity, though not necessarily eliminate it entirely. These methods provided a basis for stable coefficient estimates.

4.	Efficiency in Variable Selection
A streamlined algorithm for Mallows’ C_p was used instead of evaluating all possible variable combinations (32,000+), assuming this modern approach would yield comparable or better results without compromising accuracy.

These assumptions aimed to enhance the robustness of the analysis while adhering to the original study’s intent and incorporating modern statistical practices.

## Reproduction Plan

Our plan to recreate the author's intentions was simple: redo all of the models using the exact methods the author used, only using modern technology like R. We divided up the tasks fairly between the teammates, making sure each one of us was tasked with researching and implementing a different model. We chose to stick with the original use of Ridge Regression and Mallows' $C_p$ Criterion, however, we expanded to using Principle Component Analysis to see if there would be any sort of deviations from the paper that could be explained. The next part of the report shows, in detail, the steps we took. 

# Implementation

```{r}
#libraries used
library("bestglm")
library(ggplot2)
library(reshape2)
library(ggpubr)

library(corrplot)
library(knitr) 
library(car)

library(MASS)
library(glmnet)
library(knitr)
library(kableExtra)

library(plotly)
library(leaps)    # For all subset regression

```


## Exploratory Data Analysis

```{r}
# pulling the data
data <- mcdonald
head(data)
```

```{r}
# Summary statistics of the dataset 
summary(data)
```
Looking at the summary statistics above for each of the parameters, our first problem arises that is described in the earlier sections. We need to standardize the data to make sure that the means and medians are closely tied to each other to lessen the variability. Thus, we move forward with centralizing the data.


```{r}
# in order to replicate the paper, we need to centralize and standardize the data before we continue
data.norm <- data
for(i in 1:ncol(data)){
  data.norm[,i] <- data[,i]-mean(data[,i])
  data.norm[,i] <- data.norm[,i]/sqrt(sum((data.norm[,i])^2))
}
```

```{r}
#Distribution of response variable
#install.packages("ggplot2")
ggplot(data, aes(x = MORT)) +
  geom_histogram(binwidth = 50, fill = "skyblue", color = "black") +
  labs(
    title = "Distribution of Mortality Rates",
    x = "Mortality",
    y = "Frequency"
  ) +
  theme_minimal()
```

The author defines the response variable, mortality, as the total age adjusted mortality rate. He uses the following formula to describe the overall mortality rate:
$$
MR = (\sum D_i)(\sum (D_i/R_i))^{-1}
$$
$D_i$ and $R_i$ represent the death and age adjusted death rates, respectively, for the $i^{th}$ category for $i = 1,2,3,4$. These categories are defined as male white, female white, male non-white, and female non-white. Thus, MR is a collective, or overall mortality rate for the general population from 1959 to 1961. Even without standardizing the response variable, we see that the distribution appears to be normal and follows a nice Gaussian curve. However, we also wish to look at any outliers that may be present in the data.

```{r} 
# Define the OLS model
ols_model <- lm(MORT ~ ., data = data.norm)

# Outlier Detection with Cook's Distance
cooks_dist <- cooks.distance(ols_model)
plot(cooks_dist, main = "Cook's Distance for Outlier Detection", ylab = "Cook's Distance", xlab = "Index")
```

The Cook’s Distance plot shows a few data points with unusually high values. These points could potentially distort the regression results if left unaddressed. While we have identified these influential observations, no immediate decision to exclude them was made to preserve the integrity of the original dataset. 

```{r} 
# Boxplot 
boxplot_data <- melt(data)
ggplot(boxplot_data, aes(x = variable, y = value)) +
  geom_boxplot(fill = "lightblue", outlier.color = "red") +
  labs(title = "Boxplot of Variables", x = "Variables", y = "Values") +
  theme_minimal()
```

```{r}
# Exclude the 'DENS' variable from the data for better visualization
data_no_dens <- data[ , !(names(data) %in% "DENS")]

# Boxplot without DENS
boxplot_data_no_dens <- melt(data_no_dens)
ggplot(boxplot_data_no_dens, aes(x = variable, y = value)) +
  geom_boxplot(fill = "lightblue", outlier.color = "red") +
  labs(title = "Boxplot of Variables (Excluding DENS)", x = "Variables", y = "Values") +
  theme_minimal()
```



```{r}
# Correlation heatmap
cor_matrix <- cor(data, use = "complete.obs") # Handle missing values
corrplot(cor_matrix, method = "circle", type = "upper", order = "AOE", title = "Correlation Heatmap")
```

```{r}
# VIF Calculation
vif_values <- vif(lm(MORT ~ ., data = data.norm))
knitr::kable(vif_values, caption = "VIF for Predictors")
```

The VIF values for some variables, particularly pollution-related predictors such as $HC$ (Hydrocarbon), $NO_x$ (Nitrogen Oxides), and $SO_x$ (Sulfur Dioxide), were extremely high, exceeding 100 in some cases. These values confirm the presence of severe multicollinearity among these variables. Ridge Regression and PCA were subsequently applied to mitigate this issue, as they are specifically designed to stabilize models in the presence of multicollinearity.


```{r}
# Scatterplot: Mortality vs. Pollution Variables
hc <- ggplot(data, aes(x = HC, y = MORT)) +
  geom_point(color = "blue") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(
    title = "Mortality vs. Hydrocarbon Pollution",
    x = "Hydrocarbon Pollution",
    y = "Mortality"
  ) +
  theme_minimal()

nox <- ggplot(data, aes(x = NOX, y = MORT)) +
  geom_point(color = "blue") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(
    title = "Mortality vs. NOx Pollution",
    x = "NOx Pollution",
    y = "Mortality"
  ) +
  theme_minimal()

so2 <- ggplot(data, aes(x = SOx, y = MORT)) +
  geom_point(color = "blue") +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  labs(
    title = "Mortality vs. SO2 Pollution",
    x = "SO2 Pollution",
    y = "Mortality"
  ) +
  theme_minimal()

ggarrange(
  hc,nox,so2
)
```

The next graph shows a scatter-plot for each of the chemicals that were collected and comparing them to the mortality rate, or the response. The purpose of this graph was to simply see what each chemical is doing the mortality rate individually. 

Looking at the top left graph, we see Mortality versus Hydrocarbon Pollution. There seems to be a slightly negative slope, indicating a negative correlation between the two. The effect is weak with a large amount of variability, so it's hard to take anything from this. 

In the next graph, we take a look at the mortality rates against the $NO_x$ pollution. The slope line appears to be basically flat, indicating no significant linear relationship b between the two.

Finally, we look at mortality rates versus the $SO_2$ pollution. Here, we see a bit more of a positive slope line appearing, indicating that higher $SO_2$ pollution levels could be associated with increased mortality rates. 

What is interesting to note is that the author himself mentions that using these chemicals in particular would not yield to anything.

::: callout
"For example $SO_2$ is highly correlated with certain types of particulates and $HC$ is closely tied to carbon monoxide and lead salts. Thus one cannot demonstrate a specific cause and effect even though the analysis quantifies the relationship."
:::

```{r}
# Group DENS into categories to simplify the plot
data$DENS_group <- cut(data$DENS, breaks = 5, labels = c("Low", "Medium-Low", "Medium", "Medium-High", "High"))

# Updated visualization with grouped DENS
ggplot(data, aes(x = HC, y = MORT, color = DENS_group)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    title = "Interaction Effect: HC and Grouped DENS on Mortality",
    x = "Hydrocarbon Pollution",
    y = "Mortality",
    color = "Density Group"
  ) +
  theme_minimal()

# Exclude the 'DENS' variable from the data for better visualization
data_no_dens <- data[ , !(names(data) %in% "DENS")]

# Boxplot without DENS
boxplot_data_no_dens <- melt(data_no_dens)
ggplot(boxplot_data_no_dens, aes(x = variable, y = value)) +
  geom_boxplot(fill = "lightblue", outlier.color = "red") +
  labs(title = "Boxplot of Variables (Excluding DENS)", x = "Variables", y = "Values") +
  theme_minimal()
```

In the original dataset, the variable DENS represents population density, which has significantly larger values compared to other predictors. This disparity in scale skews the boxplot, overshadowing the distribution of other variables.
Analysis results indicate that the variable DENS (Population Density) exhibits significantly larger values compared to other predictors, causing the boxplot to overshadow the distribution of other variables. Upon excluding the DENS variable, the boxplot highlights multiple outliers in pollution-related predictors such as $HC$ (Hydrocarbon), $NO_x$ (Nitrogen Oxides), and $SO_x$ (Sulfur Oxides). These outliers suggest potential multicollinearity and underscore the need for further analysis to properly interpret the relationships between these variables and mortality rates.


## The Full Model

As is the case with any sort of Multiple Linear Regression Analysis, the first thing we as statisticians do is create and summarize the results of the full model. Below is the output.

```{r}
#first full model
ols_model <- lm(MORT ~., data = data.norm)
summary(ols_model)
```
From the output, we see that the model would look something like:
$$
\begin{align}
\hat{MORT}&= 3.375 e^{-17} + 0.3058(PREC)-0.3167(JANT)-0.2374(JULT)- 0.2134(OVR65)-0.2323(POPN)\\
&-0.2331(EDUC)-0.05382(HOUS)+0.08416(DENS)+0.6396(NONW)-0.01387(WWDRK)\\
&-0.01121(POOR)-0.9938(HC)+0.9981(NO_x)+0.08789(SO_2)+0.00922(HUMID)
\end{align}
$$
Thankfully, our full model coefficients appear to be exactly the same as the author's, so we are good to move on and take a look at the assumptions.


```{r}
ggplot(data = data.frame(
  residuals = residuals(ols_model),
  fitted = fitted(ols_model)
), aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Residuals vs. Fitted Values",
    x = "Fitted Values",
    y = "Residuals"
  ) +
  theme_minimal()
```
```{r}
qqnorm(ols_model$residuals)
qqline(ols_model$residuals)
shapiro.test(ols_model$residuals)
```
Based off of the residuals plot, we do not see any patterns present. Moving onto the qq-plot, we do some slight curvature towards the end of the line, maybe indicating that the addition of a squared value in the predictors could be useful. The Shapiro Wilk's test is satisfied; our assumptions are met, with some deviation in the linear aspect.


## Ridge Regression

### Find the Optimal Lambda 

```{r}


Y <- data.norm$MORT
X <- as.matrix(data.norm[ , !names(data.norm) %in% "MORT"])


#Cross-Validation
cv_ridge <- cv.glmnet(X, Y, alpha = 0) 
optimal_lambda <- cv_ridge$lambda.min

optimal_lambda
```

Cross-validation was used to identify the optimal penalty parameter $\lambda$. This was done with the use of the cv.glmnet function, which helped the balance between bias and variance trade-offs. 

- This process directly addresses the instability of coefficients observed in the OLS model.


### Fit Ridge Regression Model
```{r}
ridge_model <- glmnet(X, Y, alpha = 0, lambda = optimal_lambda)

ridge_coefficients <- coef(ridge_model)

ridge_coefficients

```


After finding the optimal $\lambda$, the Ridge Regression model was fit using glmnet. This helped with stabilizing coefficients, it was particularly notable with high collinear predictors (e.g. NOX, HC). Reducing the influence of these variables without eliminating them. 

- \textbf{Key Result:} Ridge Regression showed coefficients with a reduced variability. Showing a stabilized model. 

### Compare OLS and Ridge Coefficients

```{r}

ols_coefficients <- coef(ols_model)

# Compare 
comparison <- data.frame(
  Variable = rownames(ridge_coefficients),
  OLS = as.numeric(ols_coefficients),
  Ridge = as.numeric(ridge_coefficients)
)

#print(comparison)
kable(comparison, theme = "Minimal") 

```

This comparision showed shrinkage of coefficients in Ridge regression. 


### Multicollinearity Analysis
```{r}
#Find VIF
vif_values <- vif(ols_model)

#put in appendix
kable(vif_values)
plot(vif_values)
```

Variance Inflation Factors (VIF) were calculated for the OLS model to quantify multicollinearity. 

- Predictors such as $NO_x$ and $HC$ exhibited very high VIF values (>100), showing there is a severe multicollinear relationship. 

- The Ridge Regression shows this issues are reduce, as shown by the stabilization of coefficients. 


### OLS Correlation Heatmap

```{r}
cor_matrix <- cor(data.norm)

ols_cor <- cor(X)
heatmap(ols_cor, symm = TRUE, main = "OLS Correlation Heatmap")
```

This heatmap showed correlation among predictors, particularly between pollution variables. These correlations take from the importance of the Ridge Regression. 


### Predictive Accuracy

```{r}
ols_predictions <- predict(ols_model, data.norm)

ridge_predictions <- predict(ridge_model, s = optimal_lambda, newx = X)

#Find MSE
ols_mse <- mean((Y - ols_predictions)^2)
ridge_mse <- mean((Y - ridge_predictions)^2)

#data.frame(Method = c("OLS", "Ridge"), MSE = c(ols_mse, ridge_mse))
kable(data.frame(Method = c("OLS", "Ridge"), MSE = c(ols_mse, ridge_mse)))
```

Model performance was evaluated using the Mean Square Error (MSE):

- OLS MSE: 0.0039 

- Ridge MSE: 0.0045


### Ridge Trace Plot

```{r}


ridge_traces <- glmnet(X, Y, alpha = 0, lambda = cv_ridge$lambda)
trace_coefficients <- as.matrix(coef(ridge_traces))

plot_data <- data.frame(
  Lambda = rep(cv_ridge$lambda, each = nrow(trace_coefficients)),
  Coefficient = as.vector(trace_coefficients),
  Variable = rep(rownames(trace_coefficients), times = length(cv_ridge$lambda))
)

ggplot(plot_data, aes(x = log(Lambda), y = Coefficient, color = Variable)) +
  geom_line() +
  labs(
    title = "Ridge Trace Plot",
    x = "Log(Optimal Lambda)",
    y = "Coefficient"
  ) +
  theme_minimal()
#note: this ridge trace plot looks like the timeline from LOKI

```

The Ridge trace plot illustrated the progression of coefficients as $\lambda$ increased. 

- As shown in the graph, variables experienced significant shrinkage, showing the model's ability to penalize less relevant predictors.

- Stabilization of more key predictors (e.g. PREC, DENS), show there relevance to the response variable with a lower $\lambda$ value. 


### Interpretation

Pollution Variables: 

- The Ridge regression showed a lowered influence of multicollinear pollution variables. Such as, NOX, HC, and SOX, aligning with the expected outcomes. 

Demographic Factors: 

- These predictors, such as, NONW (percent nonwhite population) and OVR65 (percent population over 65) showed retention in their significance. Which showed an emphasis in socioeconomic and age-related disparities in mortality. 

Bias-Variance:

- With the use of Ridge regression, a slight bias was introduced as well. This can be seen in the increased MSE value, but also with a slightly reduced variance. This trade off helped enhance the robustness and readability, especially with the presence of multicollinearity. 

Conclusion: 

- This Ridge regression effectively lessened the multicollinearity, showing a stabilization of coefficient estimates and improved model. 


## Variable Selection: Mallow's $C_p$

## The Background

In order to start the process of eliminating variables, the author introduces Mallows' $C_p$. This statistics estimates the sum of squared biases plus the squared random errors in the response variable at all the data points. The statistic is calculated using:
$$
C_p = \dfrac{RSS_p}{\hat{\sigma^2}} - (n- 2p)
$$
Note, that $p$ is equal to the number of parameters, which includes the response variable. $RSS_p$ is the residual sum of squares for a particular regression being considered, and $\hat{\sigma^2}$ is the MSE value, or the mean square value of the given regression. 

The author goes into length about how he was able to calculate the statistic by using a certain algorithm described in *Technometrics: Computational Efficiency in the Selection of Regression Variables*[@technometrics1], an article written by LaMotte and Hocking. This algorithm takes over 32,000 total possible regression and looks at the best equations from these. His result was that they found approximately 4.5% of the 32,000 possibilities to yield a low $C_p$ value. Using modern technology, this algorithm is automatically implemented for us by using the function `regsubsets`. By using this function, we're able to quickly fish through the different $C_p$ values and find the best fitting model. Essentially, we would like to find a $C_p$ value that is close to the given number of parameters for that model. For example, the paper notes that their minimum $C_p$ value was 3.55, so they picked the model with the "best" set of five variables. Using this idea, we construct our own $C_p$ and conduct a test.

### Choosing the Number of Predictors

```{r}
#plotting mallow's Cp



#separating the data for response and predictors
response <- data.norm$MORT
predictors <- names(data.norm)[names(data.norm) != "MORT"]

#Step 1: use subsets regression to perform cp mallows calculations
subs_reg <- regsubsets(response ~ ., data = data.norm, nvmax = length(predictors))
subsets_summary <- summary(subs_reg)

#Step 2: get mallows cp and determined number of preds
cp_dat <- data.frame(num_pred = 1:length(subsets_summary$cp) - 1, cp = subsets_summary$cp)

#Step 3: create two new variables: the number of parameters plus the intercept, and then the difference between the cp value and the parameter variable. We want to pick the model that will have the number of predictors be very close to the CP value
cp_dat$parameters <- cp_dat$num_pred + 1 # Include the intercept
cp_dat$cp_diff <- abs(cp_dat$cp - cp_dat$parameters) # Difference between Cp and p
best_cp_row <- cp_dat[which.min(cp_dat$cp_diff), ] # Row with the best match

# Step 4: Visualize Mallows' Cp
ggplot(cp_dat, aes(x = num_pred, y = cp)) +
  geom_point() +  # Plot all points
  geom_line() +   # Connect points with a line
  geom_point(data = best_cp_row, aes(x = num_pred, y = cp),
             color = "red", size = 3) +  # Highlight the desired point
  labs(
    title = paste("Mallows' Cp Plot (Highlighting 15 Predictors"),
    x = "Number of Predictors",
    y = "Mallows' Cp"
  ) +
  theme_minimal()


```
In the plot above, I've constructed a simple plot that will show us what number of predictors is ideal according to Mallows' $C_p$. I determined the absolute difference between the $C_p$ value and the number of parameters, making sure to take into account the intercept. The algorithm was then able to spit out the most ideal value, and tell us what model we should use that has the right number of predictors. As we can see above, we want to use a model that has 13 predictor values, that does not include our intercept.

```{r}

# Step 3: Sort by Difference
cp_table <- cp_dat[order(cp_dat$cp_diff), ] # Sort by smallest difference

# Step 4: Display Table
knitr::kable(cp_table, col.names = c(
  "Number of Predictors", "Mallows' Cp", "Parameters (p+1)", "Difference (|Cp - Parameters|)"
), caption = "Mallows' Cp Table with Differences", format = "html") # Adjust format if not in markdown

```

The table above shows us first, the number of predictors in a given model, the $C_p$ value for that model, the number of parameters that would be in the model, as in including the intercept, and then finally, the absolute value for the difference of the statistic value and the number of parameters. We can see that the best model will be one that has 14 parameters in total, or 13 predictor variables and 1 intercept variable. That is because the difference value is the smallest. Thus, we can move on to determining which model this is and looking at the analysis of it.

### The Reduced Model

```{r}
#Step 1: find out the optimal number of coefficients according to CP, then use that to create a variable for the predictors
min_cp_index <- which.min(cp_dat$cp_diff)
best_predictors <- names(coef(subs_reg, id = min_cp_index))
best_predictors <- best_predictors[best_predictors != "(Intercept)"]

# Step 2: create our reduced model formula using the above predictors
red_cp_formula <- as.formula(paste("MORT ~", paste(best_predictors, collapse = " + ")))

# Step 3: fit reduced model using normalized data and spit out summary
red_cp_model <- lm(red_cp_formula, data = data.norm)
summary(red_cp_model)
```
We have now calculated our reduced model. In this case, the Mallows' $C_p$ value decided that the model that was "best" would be one without two predictor variables: the percent employment in white-collar occupations in 1960, or WWDRK, and the percent of families with income under 3,000 dollars in 1960, or POOR. The model looks like this: 
$$
\begin{align}
\hat{MORT}&= 3.195 e^{-17} + 0.3093(PREC)-0.3254(JANT)-0.2388(JULT)- 0.2196(OVR65)-0.2312(POPN)\\
&-0.2434(EDUC)-0.0475(HOUS)+0.08338(DENS)+0.6312(NONW)\\
&-1.0078(HC)+1.017(NO_x)+0.08789(SO_2)+0.0132(HUMID)
\end{align}
$$
This seems to make sense as in the full model, both these coefficients had low partial t-test p-values, indicating little significance to the model. We also get an $R^2$ value equal to 0.7648, meaning that our reduced model using $C_p$ actually performed better than the authors. I am not entirely sure why my model appeared to do better than the authors; the author took the time to run every single time of model and the various combinations, but perhaps that was not needed. 

### Reduced Model Assumptions

Now let's see if our reduced model using Mallows' $C_p$ checks the assumptions.


```{r}
#checking the residuals for the reduced CP Model
ggplot(data = data.frame(
  residuals = residuals(red_cp_model),
  fitted = fitted(red_cp_model)
), aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(
    title = "Residuals vs. Fitted Values",
    x = "Fitted Values",
    y = "Residuals"
  ) +
  theme_minimal()
```
```{r}
qqnorm(red_cp_model$residuals)
qqline(red_cp_model$residuals)

shapiro.test(red_cp_model$residuals)
```
Our assumptions still appear to be met with the reduced model, so we can move forward to check on the multi-collinearity.


```{r}

data_reduced <- data.norm[, best_predictors]
cor_matrix <- cor(data_reduced)

cor_data <- melt(cor_matrix)
ggplot(cor_data, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1, 1),
    name = "Correlation"
  ) +
  labs(
    title = "Correlation Plot for Predictors in Reduced Model",
    x = "Predictors",
    y = "Predictors"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    axis.text.y = element_text(size = 10)
  )
```

As expected, multi-collinearity stayed approximately the same. While the removal of two variables did make the graph slightly better, there still is a good amount of variables present, especially ones that are highly correlated together such as the chemical pollution variables. Finally, we compare the AIC values of the full model versus the reduced model, using Mallows' $C_p$, to see if the reduced model performs better.

```{r}
#compare to full model - OLS model

cat("AIC of full model:", AIC(ols_model), "\n")
cat("AIC of reduced model using Mallow's:", AIC(red_cp_model), "\n")

```

Thankfully, it appears that the reduced model is actually performing better than the full model as the AIC value is lower. It does appear as though getting rid of WWDRK and POOR appeared to help the model perform better. While I did deviate from the author's original algorithm for variable selection, I do believe I ran a more efficient code: running through over 32,000 different models to only get a lower performing model did not seem plausible to me, and while my code itself may not be as thorough as the author's, it definitely is more efficient and therefore less computationally intense for someone who would want to recreate this code. 


## Principal Component Regression (PCR)

### Motivation for PCR

In a linear regression model, when the predictors ($X_i$s) are linearly dependent or highly correlated, we say that multi-collinearity exists in the model, which causes the following issue for the fitted multiple regression model:

-   The estimate of any parameter, say $\hat{\beta_2}$, depends on all the variables included in the model. For example, $\hat{\beta_2}$ does not merely reflect the effect of the $X_2$ variable on the response variable $Y$. Due to multi-collinearity, the other variables affect the estimate of the parameter $\beta_2$. As a result, our inference about the effect of each predictor on the response is not reliable.

We have 15 predictors in our dataset, and we want to investigate the relationship between them and the response variable (mortality). There is a high correlation between most of the predictors. @fig-corr-xx illustrates this:

```{r}
#|warning: false
#------------------Cleaning the memory
rm(list=ls())
#------------------Reading Data

data <- data.frame(mcdonald)
#-----------------Centralize-Normalize Data
data.norm <- data
for(i in 1:ncol(data)){
  data.norm[,i] <- data[,i]-mean(data[,i])
  data.norm[,i] <- data.norm[,i]/sqrt(sum((data.norm[,i])^2))
}
#----------------Separating X and Y(Mort)
x.norm <- data.norm
mort.norm <- x.norm$MORT
x.norm$MORT <- NULL
#----------------Correlation Plot for predictors (X)
res_xx <- cor(x.norm, method="pearson")

```

```{r}
#| label: fig-corr-xx
#| fig-cap: Correlation plot for 15 predictors of the data set
corrplot(res_xx, method = 'ellipse', order = 'AOE', type = 'upper')
```

The correlation plot of the predictors indicates there is Multicolleniarity among predictors. For overcoming this issue the authors of the paper used Ridge Regression method and after exploring and comparing several Ridge and Ordinary Least Square models (Multiple Regression Models) they conclude that:

::: callout
"In summary, the ridge regression coefficients obtained at $k = o.2$ for the fifteen explanatory variables listed in Table IV, with one notable exception($NO_x$ ), appear to be reasonable values upon which to base a quantification of the association between these variables and the total mortality rate."
:::

In the following, we present a Principal Component Regression model with all fifteen predictors as an alternative model for overcoming multi-collinearity and compare it with the final Ridge Regression model suggested by the authors.

### Modeling PCR

In PCR, we try to provide a weighted linear combination of the predictors ($X_i$s) like: $$Z_k = v_{k1}X_1 + v_{k2}X_2 + \dots + v_{kp}X_p$$ We then replace the correlated variables $[X_1, X_2, \dots, X_p]$ with $Z = [Z_1, Z_2, \dots, Z_p]$ which are uncorrelated, so the regression model $y = Z\beta_z + \epsilon$ does not suffer from multi-collinearity.

First, we need to centralize and normalize all the variables for PCR. In the following, we assume the $y$ and $X$ variables are all centralized and normalized and show how to find the $Z$ matrix.

$$\text{Correlated: $X=[X_1, X_2, \dots, X_p]$ $\xrightarrow{\text{PC Analysis}}$ Uncorrelated: $Z=[Z_1, Z_2, \dots, Z_p]$}$$

We call $Z_i$ the i-th **principal component** of matrix $X$.

In ordinary least squares,

$$\hat{\beta_x}=\left( X'X \right)^{-1}X'y$$

We perform PCA on matrix $X$ through singular value decomposition. We obtain $$UDV'=X$$ $U$ is the left singular vectors matrix, $D$ is a diagonal matrix consisting of non-negative singular values, and $V$ is the matrix of the right singular vectors matrix (eigenvectors matrix). We calculate matrix $$Z=XV$$

The columns of $Z$ are linearly independent. If we define

$$\hat{\beta_z}=\left( Z'Z \right)^{-1}Z'y$$ then:

$$\hat{y}=X\hat{\beta_x}=Z\hat{\beta_z}$$ So, We have the regression model $y = Z\beta_z + \epsilon$ without multilinearity, but gives the same predictions of the intial model $y = X\beta_x + \epsilon$.

### Principal component Matrix $Z$, for our data.

```{r}
x.pca1 <- prcomp(x.norm)
z <- x.pca1$x
head(z)
```

### Principal components are uncorrolated:

```{r}
#| label: fig-cor-pc
#| fig-cap: Correlation plot for 15 Principal Components (Columns of the matrix Z)
res_zz <- cor(z, method="pearson")

corrplot(res_zz, method = 'ellipse', order = 'AOE', type = 'upper')
```

One question is: after converting the matrix 
$X$ to $Z$, how much of the information (variance) from the original matrix $X$ does the principal component matrix $Z$ retain?

```{r}
variation <- 100*cbind(x.pca1$sdev/sum(x.pca1$sdev),rep(0    ,15),cumsum(x.pca1$sdev)/sum(x.pca1$sdev))
colnames(variation) <- c("Var(%)","   ","CumVar(%)")
rownames(variation) <- c("PC1","PC2","PC3","PC4","PC5","PC6","PC7","PC8","PC9","PC10","PC11","PC12","PC13","PC14","PC15")
round(variation[,c(1,3)],1)
```

The first principal component ($PC_1$) only accounts $17$% of the variation (information) of $X$ matrix, the first two principal components ($PC_1$ and $PC_2$) account $30$% of the $X$ matrix variation and so on. All the principal components matrix,$Z=[PC_1, PC_2,\dots,PC_{15}]$, account $100$% of the variation of of the $X$ matrix.

```{r}
#| label: fig-scree
#| fig-cap: The scree plot show the cumulative variance explained by principal components.
p <- ncol(x.norm)
PVE <- rep(NA,p)
for(i in 1:15){ PVE[i]<- variation[i,3] }
barplot( PVE, names.arg = 1:p, main = "scree plot", 
         xlab = "number of PCs", 
         ylab = "% of variance explained" )
```

### Full PCR Model

In the following we have fitted the Principal Component Regression model ($\hat{y} = Z\hat{\beta_z}$) for our data.

```{r}
data_pcr <- data.frame(cbind(z,mort.norm))
full_pcr <- lm(mort.norm ~.,data=data_pcr)
summary(full_pcr)
```

This model has exactly the same prediction values, residual sum of squares, and $R^2$ as the Ordinary Least Squares model with the 15 predictors. However, there are some insignificant coefficients in this model. By following [@christensen2019statistical], we can drop the predictors corresponding to insignificant coefficients and build the PCR based on the predictors with significant effects.

### Reduced PCR Model

The principal components with significant effects in the full model are:

$$PC_1, PC_3, PC_6, PC_7, PC_9, PC_{12}$$ These subsets of the principal components account for `{r} round(sum(variation[c(1,3,6,7,9,12),1]))`% of the variation in the original predictors ($X_i$s). We may say that the other 49% of the variation is not significantly related to the response variable.

So the reduced model is:

```{r}
reduced_pcr <- lm(mort.norm ~PC1+PC3+PC6+PC7+PC9+PC12,data=data_pcr)
summary(reduced_pcr)
```

$$\hat{MORT}= 0.251PC_1-0.319PC_3+0.339PC_6 -0.279PC_7+0.3PC_9+0.470PC_{12} $$ We know each PC is a linear combination of all the original predictors, as follow:

```{r}
v<- x.pca1$rotation[,c(1,3,6,7,9,12)]
v
```

### Model Assumption for PCR

```{r}
#| label: fig-residual-ey
#| fig-cap: Residual versus fitted values of reduced PCR model
plot(reduced_pcr$fitted.values,reduced_pcr$residuals, xlab = "fitted values for reduced PCR model", ylab = "residuals of the reduced PCR model")
abline(h=0,lty=2,col="blue")
```

```{r}
#| label: fig-hist-pcr
#| fig-cap: Histogram for the residuals of the reduced PCR model
hist(reduced_pcr$residuals,breaks=13,xlab = "residuals of the reduced PCR model")
```

```{r}
#| label: fig-qq-pcr
#| fig-cap: Normal Q-Q plot for the residuals of the reduced PCR model
qqnorm(reduced_pcr$residuals)
qqline(reduced_pcr$residuals)
shapiro.test(reduced_pcr$residuals)
```


It seems the residuals of the reduced PCR model does not violate the assumptions.

### Final PCR Model in Terms of the Original Predictors

The reduced model meets all the model assumptions and we consider it as the final PCR model.

we can also rewrite the model in terms of the original predictors:

$$\hat{\beta_x}=V\hat{\beta_z}$$ where V is the vector of selected eigenvectors corresponding to selected principal components($[PC_1, PC_3, PC_6, PC_7, PC_9, PC_{12}]$)

```{r}
beta_x <- as.matrix(v)%*%as.matrix(reduced_pcr$coefficients)[-1,]
beta_x
```

$$
\begin{align}
\hat{MORT}&=0.224(PREC)-0.275(JANT)-0.134(JULT)+0.032(OVR65)-0.008(POPN)\\
&-0.350(EDUC)-0.071(HOUS)+0.129(DENS)+0.574(NONW)+0.574(WWDRK)\\
&+0.014(POOR)+0.003(HC)+0.012(NO_x)+0149(SO_2)+0.085(HUMID)
\end{align}
$$

For this model $R^2=0.712$ which is 14% larger (better) than the corresponding Ridge model (with k=0.2). In terms of residual sum of squared, the ridge model slightly works better than PCR. The residual sum of squared for PCR model is $\phi^*=0.287$ , which is slightly larger than the residual sum of squares for the Ridge model. The coefficients estimated for predictors are not much different from the corresponding ones in the Ridge model, except for OVR65, WWDRK, and HC, which have different signs in the two models. For OVR65 and HC, the difference in sign may not be a concern, as the estimated coefficients for these two predictors in both models are close to zero and do not significantly affect the models. However, for WWDRK, the PCR model considers that WWDRK has a positive moderate effect on mortality, while the Ridge model considers a negligible effect of -0.034.

# Results

## Comparison of Outputs

The outputs from the various models were compared using AIC, R-squared, and Mean Squared Error (MSE) as performance metrics. Below is a summary table:

```{r}
# R² and AIC values
comparison <- data.frame(
  Model = c("OLS Full", "Ridge", "Mallows' C_p", "PCR"),
  R2 = c(0.7648, 0.545, 0.7648, 0.712),     
  AIC = c(-128.2472, NA, -132.2294, NA),     
  MSE = c(0.0039, 0.0045, NA, NA)        
)

# Output the table

kable(
  comparison,
  caption = "Comparison of Model Outputs: AIC, R², and MSE",
  format = "html"
)

```



The Mallows' Cp model performed the best with the lowest AIC (-132.2294) and the highest R² (0.7648). 
Ridge Regression stabilizes coefficients but has a higher MSE (0.0045) and a lower R² (0.5450). 
The PCR model shows a relatively good R² (0.7120), but lacks AIC and MSE data for a complete evaluation.


## Deviations

As mentioned throughout the report, our group did a few things differently than the author. Our first deviation lied in how we experimented with $C_p$ Mallows'. Rather than going through and finding more than 32,000 different models and running our test that way, we used a simple construction of the algorithm to test the number of predictions, rather than each model for each number of predictors. This was more time efficient, and while our results are not the same, it did end up getting a better $R^2$ value. The $C_p$ Mallows' model also appeared to match the results of the PCR model and the Ridge Regression model, therefore keeping consistency throughout the project.

Another thing we did differently was introducing the idea of PCR, or principle component regression. This is not something the author did in his original article, but we as a team believed this method best addressed the issue of multi-collinearity without the addition of new terms like interactions, or removal of terms. 

Finally, our team did take the time to introduce some exploratory data analysis, which the author failed to do in his article. While this is a minor change, we believe introducing helpful graphs and descriptive statistics would help anyone reading this report or anyone trying to conduct their own analysis on the data see what the data itself looks like and get a better idea of how to conduct their own analysis. 

## Challenges

There were a plethora of challenges we faced throughout this project. For one, having to figure out how the data was centralized was the biggest step. In order to even start the project and make our deviations, we needed to make sure our full model, or the OLS model, matched the authors. When first creating the code for an OLS model, we made the mistake of not centralizing the data, thus getting extremely different results for the ridge regression, OLS, and $C_p$ Mallows' models. After debriefing with the team, we realized our crucial mistake, and had to go back and fix the original data to match the data the author used. The author had very briefly mentioned this step, leading to our oversight. 

The next main challenge was conducting our analysis in a way that would match the authors. This main challenge was especially prevalent in the $C_p$ Mallows' model, and led to the deviation of choosing to simply use this method another way instead of how the author did it. This leads to the main problem we faced in the project: trying to stay as authentic and close to the article as possible, while also realizing that there are better ways of conducting tests that are different from the way he did it. 

Finally, one of the biggest challenges we faced was getting GitHub to cooperate. I will not go too much into this topic, however, GitHub on RStudio was difficult and most of the time, we had to separately paste our own work into different files as committing, pushing, and pulling did not work well for us. 

# Conclusion

## Summary

This project replicated and extended the analysis from the article “Instabilities of Regression Estimates Relating Air Pollution to Mortality” using modern statistical techniques and computational tools. The original study aimed to model the relationship between air pollution and mortality, addressing challenges like multicollinearity. While adhering to the original methodology, we introduced Ridge Regression, Mallows’ $C_p$, and Principal Component Regression (PCR) to improve the analysis and enhance computational efficiency.

Key findings include that Ridge Regression stabilized coefficients, particularly for multicollinear variables like $NO_x$ and $HC$, improving variance with a slight trade-off in bias, though its predictive accuracy (R²) lagged behind Mallows’ $C_p$ and OLS models. Mallows’ $C_p$, implemented with an optimized algorithm, avoided exhaustive computation of over 32,000 models, producing a reduced model with the highest R² (0.7648) and the lowest AIC. PCR addressed multicollinearity through dimensionality reduction, offering similar predictive accuracy to OLS but with a marginally lower R² (0.712), highlighting an alternative approach to stabilize coefficients and simplify models.

Key contributions and deviations compared to the original study include refining Mallows’ $C_p$ selection for improved computational efficiency, introducing PCR for modern multicollinearity management, and incorporating exploratory data analysis to provide additional insights into data patterns and relationships. Despite these advancements, challenges such as persistent multicollinearity among pollution variables (HC, NO_x, and SO_x) remain unresolved, confirming the original study's critique of the interdependence of these pollution variables.

## Acknowledgements

We'd like to first start off by thanking Dr. Landeros for the opportunity to recreate an article as profound as this one. While time was limited, and patience was tested via RStudio, this was an incredibly fruitful project that taught students how to dive deeper into statistical computing. Based off of the lectures in STAT206, we were able to conduct a well structured analysis on older data that gave us results similar, if not better, results than the original. Our code, while sometimes robust, has better formatting and is more structured than what it would have been without the helpful tips from our professor. We thank you for taking the time to read our report. 
